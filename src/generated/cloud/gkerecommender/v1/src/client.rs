// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Code generated by sidekick. DO NOT EDIT.
#![allow(rustdoc::redundant_explicit_links)]
#![allow(rustdoc::broken_intra_doc_links)]

/// Implements a client for the GKE Recommender API.
///
/// # Example
/// ```
/// # tokio_test::block_on(async {
/// # use google_cloud_gkerecommender_v1::client::GkeInferenceQuickstart;
/// let client = GkeInferenceQuickstart::builder().build().await?;
/// // use `client` to make requests to the GKE Recommender API.
/// # gax::client_builder::Result::<()>::Ok(()) });
/// ```
///
/// # Service Description
///
/// GKE Inference Quickstart (GIQ) service provides profiles with performance
/// metrics for popular models and model servers across multiple accelerators.
/// These profiles help generate optimized best practices for running inference
/// on GKE.
///
/// # Configuration
///
/// To configure `GkeInferenceQuickstart` use the `with_*` methods in the type returned
/// by [builder()][GkeInferenceQuickstart::builder]. The default configuration should
/// work for most applications. Common configuration changes include
///
/// * [with_endpoint()]: by default this client uses the global default endpoint
///   (`https://gkerecommender.googleapis.com`). Applications using regional
///   endpoints or running in restricted networks (e.g. a network configured
//    with [Private Google Access with VPC Service Controls]) may want to
///   override this default.
/// * [with_credentials()]: by default this client uses
///   [Application Default Credentials]. Applications using custom
///   authentication may need to override this default.
///
/// [with_endpoint()]: super::builder::gke_inference_quickstart::ClientBuilder::with_endpoint
/// [with_credentials()]: super::builder::gke_inference_quickstart::ClientBuilder::credentials
/// [Private Google Access with VPC Service Controls]: https://cloud.google.com/vpc-service-controls/docs/private-connectivity
/// [Application Default Credentials]: https://cloud.google.com/docs/authentication#adc
///
/// # Pooling and Cloning
///
/// `GkeInferenceQuickstart` holds a connection pool internally, it is advised to
/// create one and the reuse it.  You do not need to wrap `GkeInferenceQuickstart` in
/// an [Rc](std::rc::Rc) or [Arc](std::sync::Arc) to reuse it, because it
/// already uses an `Arc` internally.
#[derive(Clone, Debug)]
pub struct GkeInferenceQuickstart {
    inner: std::sync::Arc<dyn super::stub::dynamic::GkeInferenceQuickstart>,
}

impl GkeInferenceQuickstart {
    /// Returns a builder for [GkeInferenceQuickstart].
    ///
    /// ```
    /// # tokio_test::block_on(async {
    /// # use google_cloud_gkerecommender_v1::client::GkeInferenceQuickstart;
    /// let client = GkeInferenceQuickstart::builder().build().await?;
    /// # gax::client_builder::Result::<()>::Ok(()) });
    /// ```
    pub fn builder() -> super::builder::gke_inference_quickstart::ClientBuilder {
        gax::client_builder::internal::new_builder(
            super::builder::gke_inference_quickstart::client::Factory,
        )
    }

    /// Creates a new client from the provided stub.
    ///
    /// The most common case for calling this function is in tests mocking the
    /// client's behavior.
    pub fn from_stub<T>(stub: T) -> Self
    where
        T: super::stub::GkeInferenceQuickstart + 'static,
    {
        Self {
            inner: std::sync::Arc::new(stub),
        }
    }

    pub(crate) async fn new(
        config: gaxi::options::ClientConfig,
    ) -> gax::client_builder::Result<Self> {
        let inner = Self::build_inner(config).await?;
        Ok(Self { inner })
    }

    async fn build_inner(
        conf: gaxi::options::ClientConfig,
    ) -> gax::client_builder::Result<std::sync::Arc<dyn super::stub::dynamic::GkeInferenceQuickstart>>
    {
        if gaxi::options::tracing_enabled(&conf) {
            return Ok(std::sync::Arc::new(Self::build_with_tracing(conf).await?));
        }
        Ok(std::sync::Arc::new(Self::build_transport(conf).await?))
    }

    async fn build_transport(
        conf: gaxi::options::ClientConfig,
    ) -> gax::client_builder::Result<impl super::stub::GkeInferenceQuickstart> {
        super::transport::GkeInferenceQuickstart::new(conf).await
    }

    async fn build_with_tracing(
        conf: gaxi::options::ClientConfig,
    ) -> gax::client_builder::Result<impl super::stub::GkeInferenceQuickstart> {
        Self::build_transport(conf)
            .await
            .map(super::tracing::GkeInferenceQuickstart::new)
    }

    /// Fetches available models. Open-source models follow the Huggingface Hub
    /// `owner/model_name` format.
    pub fn fetch_models(&self) -> super::builder::gke_inference_quickstart::FetchModels {
        super::builder::gke_inference_quickstart::FetchModels::new(self.inner.clone())
    }

    /// Fetches available model servers. Open-source model servers use simplified,
    /// lowercase names (e.g., `vllm`).
    pub fn fetch_model_servers(
        &self,
    ) -> super::builder::gke_inference_quickstart::FetchModelServers {
        super::builder::gke_inference_quickstart::FetchModelServers::new(self.inner.clone())
    }

    /// Fetches available model server versions. Open-source servers use their own
    /// versioning schemas (e.g., `vllm` uses semver like `v1.0.0`).
    ///
    /// Some model servers have different versioning schemas depending on the
    /// accelerator. For example, `vllm` uses semver on GPUs, but returns nightly
    /// build tags on TPUs. All available versions will be returned when different
    /// schemas are present.
    pub fn fetch_model_server_versions(
        &self,
    ) -> super::builder::gke_inference_quickstart::FetchModelServerVersions {
        super::builder::gke_inference_quickstart::FetchModelServerVersions::new(self.inner.clone())
    }

    /// Fetches available profiles. A profile contains performance metrics and
    /// cost information for a specific model server setup. Profiles can be
    /// filtered by parameters. If no filters are provided, all profiles are
    /// returned.
    ///
    /// Profiles display a single value per performance metric based on the
    /// provided performance requirements. If no requirements are given, the
    /// metrics represent the inflection point. See [Run best practice inference
    /// with GKE Inference Quickstart
    /// recipes](https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/inference-quickstart#how)
    /// for details.
    pub fn fetch_profiles(&self) -> super::builder::gke_inference_quickstart::FetchProfiles {
        super::builder::gke_inference_quickstart::FetchProfiles::new(self.inner.clone())
    }

    /// Generates an optimized deployment manifest for a given model and model
    /// server, based on the specified accelerator, performance targets, and
    /// configurations. See [Run best practice inference with GKE Inference
    /// Quickstart
    /// recipes](https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/inference-quickstart)
    /// for deployment details.
    pub fn generate_optimized_manifest(
        &self,
    ) -> super::builder::gke_inference_quickstart::GenerateOptimizedManifest {
        super::builder::gke_inference_quickstart::GenerateOptimizedManifest::new(self.inner.clone())
    }

    /// Fetches all of the benchmarking data available for a profile. Benchmarking
    /// data returns all of the performance metrics available for a given model
    /// server setup on a given instance type.
    pub fn fetch_benchmarking_data(
        &self,
    ) -> super::builder::gke_inference_quickstart::FetchBenchmarkingData {
        super::builder::gke_inference_quickstart::FetchBenchmarkingData::new(self.inner.clone())
    }
}
